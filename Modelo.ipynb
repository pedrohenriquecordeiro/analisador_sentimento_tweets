{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = pd.read_csv('treated_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73288</th>\n",
       "      <td>camis bonit ha d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260172</th>\n",
       "      <td>ent tod :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393637</th>\n",
       "      <td>vide fim lind demal saudad aument :( lubet segred</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302181</th>\n",
       "      <td>aind :( jant daqu pouc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181367</th>\n",
       "      <td>resp fund prior xd tir palit p</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet_text  sentiment\n",
       "73288                                    camis bonit ha d          1\n",
       "260172                                         ent tod :(          0\n",
       "393637  vide fim lind demal saudad aument :( lubet segred          0\n",
       "302181                             aind :( jant daqu pouc          0\n",
       "181367                     resp fund prior xd tir palit p          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando o TF-IDF e o 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase = False , ngram_range = (1,3))\n",
    "data_vectorized = tfidf.fit_transform(\n",
    "    dataset.tweet_text.apply(lambda tweet: np.str_(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_vectorized\n",
    "y = dataset.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newton-cg-100\n",
      "lbfgs-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/pedro/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "def test(solver,max_iter):\n",
    "    \n",
    "    cv = KFold(n_splits = 4)\n",
    "    \n",
    "    modelo = LogisticRegression(\n",
    "        solver = solver, \n",
    "        multi_class='auto',\n",
    "        verbose = 0,\n",
    "        max_iter = max_iter)\n",
    "    \n",
    "    results = cross_validate(modelo, \n",
    "                            x, y,\n",
    "                            cv = cv, \n",
    "                            groups = dataset.sentiment, \n",
    "                           return_train_score = True)\n",
    "    \n",
    "    # salva os resultados\n",
    "    result.append(\n",
    "      [\n",
    "        solver,\n",
    "        max_iter,\n",
    "        results['fit_time'].mean(),\n",
    "        results['score_time'].mean(),\n",
    "        results['train_score'].mean() * 100,\n",
    "        results['test_score'].mean() * 100\n",
    "      ])\n",
    "  \n",
    "  \n",
    "for max_iter in range(100, 1000,200):\n",
    "    for solver in ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']:\n",
    "        print('%s-%d' % (solver,max_iter))\n",
    "        test(solver,max_iter)\n",
    "\n",
    "result = pd.DataFrame(result , \n",
    "                      columns = [ 'solver', \n",
    "                                 'max_iter', \n",
    "                                 'fit_time', \n",
    "                                 'score_time', \n",
    "                                 'train_score' , \n",
    "                                 'test_score'])\n",
    "\n",
    "# mostra os melhores resultados\n",
    "result.sort_values('test_score' , ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "def test(loss,max_iter):\n",
    "    \n",
    "    cv = KFold(n_splits = 4)\n",
    "    \n",
    "    modelo = LinearSVC(\n",
    "        loss = loss \n",
    "        dual = False\n",
    "        random_state = 0,\n",
    "        max_iter = max_iter)\n",
    "    \n",
    "    results = cross_validate(modelo, \n",
    "                            x, y,\n",
    "                            cv = cv, \n",
    "                            groups = dataset.sentiment, \n",
    "                           return_train_score = True)\n",
    "    \n",
    "    # salva os resultados\n",
    "    result.append(\n",
    "      [\n",
    "        loss,\n",
    "        max_iter,\n",
    "        results['fit_time'].mean(),\n",
    "        results['score_time'].mean(),\n",
    "        results['train_score'].mean() * 100,\n",
    "        results['test_score'].mean() * 100\n",
    "      ])\n",
    "  \n",
    "  \n",
    "for max_iter in range(100, 1000,200):\n",
    "    for loss in ['hinge','squared_hinge']:\n",
    "        print('loss:%s-max_iter:%d' % (loss,max_iter))\n",
    "        test(loss,max_iter)\n",
    "\n",
    "result = pd.DataFrame(result , \n",
    "                      columns = [ 'loss', \n",
    "                                 'max_iter', \n",
    "                                 'fit_time', \n",
    "                                 'score_time', \n",
    "                                 'train_score' , \n",
    "                                 'test_score'])\n",
    "\n",
    "# mostra os melhores resultados\n",
    "result.sort_values('test_score' , ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "def test(max_depth,min_samples_split,min_samples_leaf,criterion):\n",
    "    \n",
    "    cv = KFold(n_splits = 4)\n",
    "    \n",
    "    modelo = DecisionTreeClassifier(\n",
    "          max_depth = max_depth,\n",
    "          min_samples_leaf = min_samples_leaf,\n",
    "          min_samples_split = min_samples_split,\n",
    "          criterion = criterion)\n",
    "    \n",
    "    results = cross_validate(modelo, x, y,\n",
    "                               cv = cv, \n",
    "                               groups = dataset.sentiment, \n",
    "                               return_train_score = True)\n",
    "\n",
    "    # salva os resultados\n",
    "    result.append([\n",
    "            max_depth,\n",
    "            min_samples_split,\n",
    "            criterion,\n",
    "            min_samples_leaf,\n",
    "            results['fit_time'].mean(),\n",
    "            results['score_time'].mean(),\n",
    "            results['train_score'].mean() * 100,\n",
    "            results['test_score'].mean() * 100 ])\n",
    "\n",
    "for max_depth in range (3, 15 + 1):\n",
    "    for min_samples_split in range (1, 10 + 1):\n",
    "        for min_samples_leaf in range (1, 10 + 1):\n",
    "            for criterion in [\"gini\", \"entropy\"]:\n",
    "                print('max_depth:%d-min_samples_split:%d-min_samples_leaf:%d-criterion:%s' \n",
    "                      % (max_depth,min_samples_split,min_samples_leaf,criterion))\n",
    "                test(max_depth,\n",
    "                     min_samples_split,\n",
    "                     min_samples_leaf,\n",
    "                     criterion)\n",
    "\n",
    "result = pd.DataFrame(result , \n",
    "                      columns = [ 'max_depth',\n",
    "                                 'min_samples_split' ,\n",
    "                                 'criterion',\n",
    "                                 'min_samples_leaf',\n",
    "                                 'fit_time', \n",
    "                                 'score_time', \n",
    "                                 'train_score' , \n",
    "                                 'test_score'])\n",
    "\n",
    "# mostra os melhores resultados\n",
    "result.sort_values('test_score' , ascending = False).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
